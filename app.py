# -*- coding: utf-8 -*-
"""FYP_17153472 (version 2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oM2l18wjUjHDag2hW4PkeVczOKDkACrA
"""

# === STEP 1: Import Libraries ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Display settings
pd.set_option('display.max_columns', None)
sns.set(style='whitegrid')

df = pd.read_csv('data_rdd.csv', encoding='ISO-8859-1')

# === STEP 3: Display First Few Rows ===
df.head()

"""# **Exploratory Data Analysis (EDA)**"""

# === STEP 4: Summary Statistics ===
df.describe()

# === STEP 5: Check for Missing Values ===
missing_values = df.isnull().sum()
print("Missing Values per Column:\n", missing_values[missing_values > 0])

# === STEP 6: Distribution Plots for Key Numerical Features ===
numerical_features = ['views', 'bubble_rating', 'score_adjusted', 'n_reviews', 'price_min', 'price_max']
for feature in numerical_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[feature], kde=True, bins=30, color='skyblue')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

# === STEP 7: Boxplots for Outlier Detection ===
for feature in numerical_features:
    plt.figure(figsize=(6, 3))
    sns.boxplot(x=df[feature], color='lightcoral')
    plt.title(f'Boxplot of {feature}')
    plt.tight_layout()
    plt.show()

# === STEP 8: Correlation Heatmap ===
plt.figure(figsize=(10, 8))
corr_matrix = df[numerical_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()
plt.show()

# === STEP 9: Top 20 Most Common Amenities ===
# Filter for binary columns (excluding numerical features)
binary_columns = df.select_dtypes(include='int64').drop(numerical_features, axis=1, errors='ignore')
top_amenities = binary_columns.sum().sort_values(ascending=False).head(20)

plt.figure(figsize=(10, 6))
sns.barplot(x=top_amenities.values, y=top_amenities.index, palette='viridis')
plt.title('Top 20 Most Common Hotel Amenities')
plt.xlabel('Count')
plt.tight_layout()
plt.show()

"""# Analyze the relationship between hotel amenities and hotel ratings and prices.

## Select amenities for analysis

### Subtask:
Choose a subset of amenities to focus on for the analysis. You can select a few of the most common amenities or amenities that you think might have a significant impact on ratings or prices.

**Reasoning**:
Based on the `top_amenities` series and common perceptions of hotel features, I will select a subset of amenities that are frequent or potentially impactful on hotel ratings and prices for further analysis.
"""

# Select a subset of amenities to focus on
selected_amenities = [
    'amenities_Free High Speed Internet (WiFi)',
    'amenities_Air conditioning',
    'amenities_Free internet',
    'amenities_Airport transportation',
    'amenities_Free breakfast',
    'amenities_Fitness Center with Gym / Workout Room',
    'amenities_Spa',
    'amenities_Restaurant',
    'amenities_Room service',
    'amenities_Pool'
]

print("Selected amenities for further analysis:", selected_amenities)

"""## Analyze relationship between amenities and ratings

### Subtask:
For each selected amenity, compare the ratings of hotels that have the amenity versus those that don't. This can be done using visualizations (like boxplots or violin plots) and statistical tests (like t-tests) if appropriate.

**Reasoning**:
Iterate through the selected amenities, create boxplots and perform t-tests to compare adjusted scores for hotels with and without each amenity.
"""

from scipy import stats

for amenity in selected_amenities:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=amenity, y='score_adjusted', data=df, palette='pastel')
    plt.title(f'Score Adjusted Distribution for Hotels with/without {amenity}')
    plt.xlabel(amenity)
    plt.ylabel('Score Adjusted')
    plt.xticks([0, 1], ['Without Amenity', 'With Amenity'])
    plt.tight_layout()
    plt.show()

    group_with_amenity = df[df[amenity] == 1]['score_adjusted'].dropna()
    group_without_amenity = df[df[amenity] == 0]['score_adjusted'].dropna()

    if len(group_with_amenity) > 1 and len(group_without_amenity) > 1:
        t_stat, p_value = stats.ttest_ind(group_with_amenity, group_without_amenity, equal_var=False)
        print(f"T-test for {amenity}: p-value = {p_value:.4f}\n")
    else:
        print(f"Not enough data to perform T-test for {amenity}\n")

"""**Reasoning**:
The previous code block successfully generated boxplots and performed t-tests for the selected amenities. The boxplots visualize the distribution of adjusted scores for hotels with and without each amenity, and the t-tests provide a statistical measure of the difference in means. The next step is to add violin plots as an alternative visualization as requested in the instructions.


"""

for amenity in selected_amenities:
    plt.figure(figsize=(8, 5))
    sns.violinplot(x=amenity, y='score_adjusted', data=df, palette='viridis')
    plt.title(f'Score Adjusted Distribution for Hotels with/without {amenity} (Violin Plot)')
    plt.xlabel(amenity)
    plt.ylabel('Score Adjusted')
    plt.xticks([0, 1], ['Without Amenity', 'With Amenity'])
    plt.tight_layout()
    plt.show()

"""## Analyze relationship between amenities and prices

### Subtask:
Similarly, for each selected amenity, compare the prices of hotels that have the amenity versus those that don't. Visualizations (like boxplots) and statistical tests can be used here as well.

**Reasoning**:
Iterate through the selected amenities and for each, create a boxplot comparing the 'price_min' distribution for hotels with and without the amenity, then perform an independent samples t-test and print the results.
"""

from scipy import stats

for amenity in selected_amenities:
    # Create boxplot
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=amenity, y='price_min', data=df.dropna(subset=['price_min', amenity]), palette='pastel')
    plt.title(f'Price Min Distribution for Hotels with/without {amenity}')
    plt.xlabel(amenity)
    plt.ylabel('Price Min')
    plt.xticks([0, 1], ['Without Amenity', 'With Amenity'])
    plt.tight_layout()
    plt.show()

    # Perform independent samples t-test
    group_with_amenity = df[df[amenity] == 1]['price_min'].dropna()
    group_without_amenity = df[df[amenity] == 0]['price_min'].dropna()

    if len(group_with_amenity) > 1 and len(group_without_amenity) > 1:
        t_statistic, p_value = stats.ttest_ind(group_with_amenity, group_without_amenity, equal_var=False) # Assuming unequal variances
        print(f"T-test for {amenity}:")
        print(f"  T-statistic: {t_statistic:.4f}")
        print(f"  P-value: {p_value:.4f}\n")
    else:
        print(f"Not enough data to perform T-test for {amenity}\n")

"""## Summarize findings

### Subtask:
Present the key insights from the analysis, highlighting which amenities appear to be associated with higher ratings or different price ranges.

**Reasoning**:
Synthesize the findings from the previous analysis steps to present the key insights about the relationship between amenities and hotel ratings and prices.
"""

print("--- Key Insights from Amenity Analysis ---")

print("\nRelationship with Adjusted Score (Ratings):")
print("-------------------------------------------")

# Based on the t-test results from the previous rating analysis step:
amenities_higher_ratings = [
    'amenities_Free High Speed Internet (WiFi)',
    'amenities_Air conditioning',
    'amenities_Free internet',
    'amenities_Airport transportation',
    'amenities_Free breakfast',
    'amenities_Fitness Center with Gym / Workout Room',
    'amenities_Spa',
    'amenities_Restaurant'
]

amenities_no_sig_ratings = [
    'amenities_Room service',
    'amenities_Pool'
]

print("Amenities associated with statistically significantly different (mostly higher) adjusted scores:")
for amenity in amenities_higher_ratings:
    print(f"- {amenity}")

print("\nAmenities that did not show a statistically significant association with adjusted scores:")
for amenity in amenities_no_sig_ratings:
    print(f"- {amenity}")


print("\nRelationship with Minimum Price:")
print("---------------------------------")

# Based on the t-test results from the previous price analysis step:
amenities_higher_prices = [
    'amenities_Free internet', # Note: While 'Free High Speed Internet (WiFi)' didn't show significance for price, 'Free internet' did.
    'amenities_Airport transportation',
    'amenities_Fitness Center with Gym / Workout Room',
    'amenities_Spa',
    'amenities_Restaurant',
    'amenities_Room service'
]

amenities_no_sig_prices = [
    'amenities_Free High Speed Internet (WiFi)',
    'amenities_Air conditioning',
    'amenities_Free breakfast',
    'amenities_Pool'
]

print("Amenities associated with statistically significantly different (mostly higher) minimum prices:")
for amenity in amenities_higher_prices:
    print(f"- {amenity}")

print("\nAmenities that did not show a statistically significant association with minimum prices:")
for amenity in amenities_no_sig_prices:
    print(f"- {amenity}")

print("\nOverall Summary:")
print("----------------")
print("Several amenities are associated with both higher adjusted ratings and higher minimum prices, including 'Free internet', 'Airport transportation', 'Fitness Center with Gym / Workout Room', 'Spa', and 'Restaurant'.")
print("'Free High Speed Internet (WiFi)' and 'Air conditioning' were associated with different ratings but not significantly different minimum prices in this analysis.")
print("'Room service' was associated with significantly different minimum prices but not significantly different ratings.")
print("'Free breakfast' and 'Pool' did not show a statistically significant association with either adjusted ratings or minimum prices in this analysis.")

"""## Summary:

### Data Analysis Key Findings

*   Amenities such as 'Free High Speed Internet (WiFi)', 'Air conditioning', 'Free internet', 'Airport transportation', 'Free breakfast', 'Fitness Center with Gym / Workout Room', 'Spa', and 'Restaurant' are associated with statistically significantly different (mostly higher) adjusted hotel ratings.
*   'Free internet', 'Airport transportation', 'Fitness Center with Gym / Workout Room', 'Spa', 'Restaurant', and 'Room service' are associated with statistically significantly different (mostly higher) minimum hotel prices.
*   Amenities like 'Free internet', 'Airport transportation', 'Fitness Center with Gym / Workout Room', 'Spa', and 'Restaurant' are associated with both higher adjusted ratings and higher minimum prices.
*   'Room service' is associated with significantly different minimum prices but not significantly different adjusted ratings.
*   'Free High Speed Internet (WiFi)' and 'Air conditioning' are associated with different ratings but not significantly different minimum prices.
*   'Free breakfast' and 'Pool' did not show a statistically significant association with either adjusted ratings or minimum prices in this analysis.

### Insights or Next Steps

*   Hotels looking to improve ratings may consider investing in amenities such as 'Free High Speed Internet (WiFi)', 'Air conditioning', 'Free internet', 'Airport transportation', 'Free breakfast', 'Fitness Center', 'Spa', and 'Restaurant'.
*   Hotels aiming for higher price points should consider offering amenities like 'Free internet', 'Airport transportation', 'Fitness Center', 'Spa', 'Restaurant', and 'Room service'.

# **Data Pre-processing**
"""

# STEP 1: Import Library
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler

# STEP 2: Handle Missing Values
df['price_curr_min'].fillna(df['price_curr_min'].median(), inplace=True)
df['price_min'].fillna(df['price_min'].median(), inplace=True)
df['price_max'].fillna(df['price_max'].median(), inplace=True)
df['location_grade'].fillna(df['location_grade'].median(), inplace=True)
df['photos'].fillna(df['photos'].mean(), inplace=True)

# STEP 3: Log Transform Highly Skewed Features
df['views_log'] = np.log1p(df['views'])
df['n_reviews_log'] = np.log1p(df['n_reviews'])
df['price_max_log'] = np.log1p(df['price_max'])

# STEP 4: Feature Selection - Remove low-variance binary features
binary_cols = df.select_dtypes(include=['int64']).drop(columns=['views_binary'], errors='ignore')
selector = VarianceThreshold(threshold=0.01)  # Remove binary features with <1% variance
binary_cols_reduced = pd.DataFrame(selector.fit_transform(binary_cols), columns=binary_cols.columns[selector.get_support()])

# STEP 5: Assemble Clean Feature Set
selected_features = [
    'score_adjusted', 'bubble_rating', 'price_curr_min', 'location_grade',
    'photos', 'discount', 'class_4_5', 'n_reviews', 'award_cert_excellence',
    'category_hotel', 'category_inn', 'category_specialty',
    'amenities_Free High Speed Internet (WiFi)', 'discount_perc'
]
joblib.dump(selected_features, 'feature_names.pkl')

# Ensure only complete rows are kept
df_model_ready = pd.concat([df[selected_features], binary_cols_reduced, df['views_binary']], axis=1)
df_model_ready.dropna(inplace=True)

# STEP 6: Normalize Features
X = df_model_ready.drop('views_binary', axis=1)
y = df_model_ready['views_binary']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# STEP 7: Show results
print("✅ Data preprocessing complete.")
print("Shape of feature matrix (X):", X_scaled.shape)
print("Shape of target variable (y):", y.shape)

"""# **Modelling**

## Random Forest Classifier
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Split dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize and train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test data
y_pred = rf_model.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

# Visualise Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Popular', 'Popular'], yticklabels=['Not Popular', 'Popular'])
plt.title('Confusion Matrix for Random Forest Classifier')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Feature Importance Plot
importances = rf_model.feature_importances_
feature_names = X.columns if hasattr(X, 'columns') else [f'Feature {i}' for i in range(X.shape[1])]
feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_imp_df.sort_values(by='Importance', ascending=False, inplace=True)

# Plot top 10 important features
plt.figure(figsize=(10, 6))
sns.barplot(data=feature_imp_df.head(10), x='Importance', y='Feature', palette='viridis')
plt.title('Top 10 Feature Importances')
plt.xlabel('Relative Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""# XGBoost Classifier"""

!pip install xgboost --quiet

import xgboost as xgb
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize and train the XGBoost model
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)

# Predict on test data
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate performance
acc_xgb = accuracy_score(y_test, y_pred_xgb)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
report_xgb = classification_report(y_test, y_pred_xgb)

print("XGBoost Accuracy:", acc_xgb)
print("Confusion Matrix:\n", cm_xgb)
print("Classification Report:\n", report_xgb)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# XGBoost Confusion Matrix
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Popular', 'Popular'], yticklabels=['Not Popular', 'Popular'])
plt.title("XGBoost Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Initialize and train Logistic Regression model
logreg_model = LogisticRegression(max_iter=1000)
logreg_model.fit(X_train, y_train)

# Predict on test data
y_pred_lr = logreg_model.predict(X_test)

# Evaluate performance
acc_lr = accuracy_score(y_test, y_pred_lr)
cm_lr = confusion_matrix(y_test, y_pred_lr)
report_lr = classification_report(y_test, y_pred_lr)

print("Logistic Regression Accuracy:", acc_lr)
print("Confusion Matrix:\n", cm_lr)
print("Classification Report:\n", report_lr)

# Logistic Regression Confusion Matrix
plt.figure(figsize=(5, 4))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Greens',
            xticklabels=['Not Popular', 'Popular'], yticklabels=['Not Popular', 'Popular'])
plt.title("Logistic Regression Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""# **Model Evaluation**"""

from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
lr = LogisticRegression(max_iter=1000, random_state=42)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=50, random_state=42)
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
lr_scores = cross_validate(lr, X_scaled, y, cv=cv, scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'])
xgb_scores = cross_validate(xgb, X_scaled, y, cv=cv, scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'])
rf_scores = cross_validate(rf, X_scaled, y, cv=cv, scoring=['accuracy', 'f1', 'precision', 'recall', 'roc_auc'])

# Fit models
lr.fit(X_train_cv, y_train_cv)
xgb.fit(X_train_cv, y_train_cv)
rf.fit(X_train_cv, y_train_cv)

y_pred_lr = lr.predict(X_test_cv)
y_pred_xgb = xgb.predict(X_test_cv)
y_pred_rf = rf.predict(X_test_cv)

print("MAE / RMSE:")
print("Logistic Regression:", mean_absolute_error(y_test_cv, y_pred_lr), np.sqrt(mean_squared_error(y_test_cv, y_pred_lr)))
print("XGBoost:", mean_absolute_error(y_test_cv, y_pred_xgb), np.sqrt(mean_squared_error(y_test_cv, y_pred_xgb)))
print("Random Forest:", mean_absolute_error(y_test_cv, y_pred_rf), np.sqrt(mean_squared_error(y_test_cv, y_pred_rf)))

# ROC Curve
y_proba_lr = lr.predict_proba(X_test_cv)[:, 1]
y_proba_xgb = xgb.predict_proba(X_test_cv)[:, 1]
y_proba_rf = rf.predict_proba(X_test_cv)[:, 1]

fpr_lr, tpr_lr, _ = roc_curve(y_test_cv, y_proba_lr)
fpr_xgb, tpr_xgb, _ = roc_curve(y_test_cv, y_proba_xgb)
fpr_rf, tpr_rf, _ = roc_curve(y_test_cv, y_proba_rf)

auc_lr = auc(fpr_lr, tpr_lr)
auc_xgb = auc(fpr_xgb, tpr_xgb)
auc_rf = auc(fpr_rf, tpr_rf)

# Visualize ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_lr, tpr_lr, label=f"Logistic Regression (AUC = {auc_lr:.2f})")
plt.plot(fpr_xgb, tpr_xgb, label=f"XGBoost (AUC = {auc_xgb:.2f})")
plt.plot(fpr_rf, tpr_rf, label=f"Random Forest (AUC = {auc_rf:.2f})")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc='lower right')
plt.grid()
plt.show()

import pandas as pd

# Convert cross-validation results to DataFrame
pd.DataFrame(lr_scores).mean()  # Logistic Regression

pd.DataFrame(rf_scores).mean()  # Random Forest

pd.DataFrame(xgb_scores).mean() # XGBoost

"""# **Deployment**"""

import joblib

joblib.dump(lr, 'logistic_model.pkl')

import streamlit as st
import joblib
import numpy as np
import pandas as pd

# Load trained model and feature names
model = joblib.load("logistic_model.pkl")
feature_names = joblib.load("feature_names.pkl")  # Must match training columns

st.title("Hotel Popularity Prediction")
st.markdown("Enter hotel details to predict its popularity:")

# Input fields for user
score_adjusted = st.number_input("Score Adjusted (e.g., 4.3)", min_value=1.0, max_value=5.0, value=4.5)
bubble_rating = st.selectbox("Bubble Rating", [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0], index=7)
price_curr_min = st.number_input("Current Minimum Price (e.g., 100)", value=100.0)
location_grade = st.number_input("Location Grade (e.g., 85)", min_value=0.0, max_value=100.0, value=90.0)
photos = st.number_input("Number of Photos", value=50)
discount = st.selectbox("Has Discount?", [0, 1])
class_4_5 = st.selectbox("Hotel Class 4-5 Star", [0, 1])

# Create user input dictionary
user_input = {
    'score_adjusted': score_adjusted,
    'bubble_rating': bubble_rating,
    'price_curr_min': price_curr_min,
    'location_grade': location_grade,
    'photos': photos,
    'discount': discount,
    'class_4_5': class_4_5
}

# Fill remaining features with 0 and ensure correct order
input_data = [user_input.get(col, 0) for col in feature_names]
input_array = np.array(input_data).reshape(1, -1)

# Predict button
if st.button("Predict Popularity"):
    prediction = model.predict(input_array)[0]
    prediction_proba = model.predict_proba(input_array)[0][1] * 100

    st.markdown("### Prediction Result")
    if prediction == 1:
        st.success(f"✅ This hotel is likely to be **popular** with {prediction_proba:.2f}% confidence.")
    else:
        st.warning(f"⚠️ This hotel is **less likely to be popular**, confidence: {100 - prediction_proba:.2f}%.")